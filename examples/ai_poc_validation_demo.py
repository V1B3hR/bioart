#!/usr/bin/env python3
"""
AI PoC Validation Demo

Demonstrates the three-pillar validation approach for AI Proof of Concepts:
1. Strategic Validation (The "Why")
2. Technical Validation (The "How" with Python) 
3. Operational Validation (The "What If")

This example follows the comprehensive methodology outlined in docs/AI_POC_VALIDATION_GUIDE.md
"""

import numpy as np
import pandas as pd
import time
import warnings
from typing import Dict, Any, Tuple
import os
import sys

# Add src directory to path for bioart imports
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# Suppress warnings for cleaner demo output
warnings.filterwarnings('ignore')

def ensure_sklearn_available():
    """Check if scikit-learn is available for the demo"""
    try:
        from sklearn.model_selection import train_test_split
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import classification_report, confusion_matrix
        from sklearn.datasets import make_classification
        return True
    except ImportError:
        print("‚ö†Ô∏è  scikit-learn not available. Installing minimal demo version...")
        return False

def create_synthetic_dataset():
    """Create a synthetic dataset for demonstration purposes"""
    print("üìä Creating synthetic customer churn dataset...")
    
    # Simulate customer data
    np.random.seed(42)  # For reproducibility
    n_samples = 1000
    
    # Features: tenure_months, monthly_charges, support_tickets
    tenure_months = np.random.exponential(24, n_samples)
    monthly_charges = np.random.normal(65, 20, n_samples)
    support_tickets = np.random.poisson(2, n_samples)
    total_charges = tenure_months * monthly_charges + np.random.normal(0, 100, n_samples)
    
    # Create churn based on logical rules (higher churn with short tenure, high charges, many tickets)
    churn_probability = (
        0.6 * (tenure_months < 12) +  # Short tenure increases churn
        0.3 * (monthly_charges > 80) +  # High charges increase churn
        0.4 * (support_tickets > 3) +  # Many tickets increase churn
        np.random.normal(0, 0.1, n_samples)  # Add noise
    )
    churn = (churn_probability > 0.7).astype(int)
    
    # Create DataFrame
    data = pd.DataFrame({
        'tenure_months': tenure_months,
        'monthly_charges': monthly_charges,
        'support_tickets': support_tickets,
        'total_charges': total_charges,
        'churn': churn
    })
    
    print(f"   Dataset created: {len(data)} samples")
    print(f"   Churn rate: {churn.mean():.1%}")
    print(f"   Features: {list(data.columns[:-1])}")
    
    return data

class MockModel:
    """Simple mock model for demonstration when sklearn is not available"""
    
    def __init__(self):
        self.is_fitted = False
        self.feature_names = None
        
    def fit(self, X, y):
        """Mock fit method"""
        self.is_fitted = True
        self.feature_names = X.columns.tolist() if hasattr(X, 'columns') else None
        # Simple rule-based prediction: churn if tenure < 12 months
        return self
        
    def predict(self, X):
        """Mock prediction using simple rules"""
        if not self.is_fitted:
            raise ValueError("Model must be fitted first")
            
        # Simple heuristic: predict churn for short tenure + high charges
        if hasattr(X, 'iloc'):
            predictions = ((X['tenure_months'] < 15) & (X['monthly_charges'] > 70)).astype(int)
        else:
            # Handle single prediction case
            predictions = np.array([1 if X[0] < 15 and X[1] > 70 else 0])
            
        return predictions
        
    def predict_proba(self, X):
        """Mock probability prediction"""
        predictions = self.predict(X)
        # Convert to probabilities
        probabilities = np.column_stack([1 - predictions, predictions])
        return probabilities

def pillar_1_strategic_validation():
    """
    Pillar 1: Strategic Validation (The "Why")
    Define success criteria, baseline, and data requirements
    """
    print("\n" + "="*60)
    print("üéØ PILLAR 1: STRATEGIC VALIDATION (The 'Why')")
    print("="*60)
    
    # 1.1 Define Crystal-Clear Success Criteria
    print("\n1.1 üìã Success Criteria Definition")
    print("-" * 40)
    
    success_criteria = {
        'precision': {
            'target': 0.70,
            'description': 'Of customers predicted to churn, 70% actually do',
            'business_impact': 'Minimizes wasted retention efforts'
        },
        'recall': {
            'target': 0.65, 
            'description': 'Catch 65% of customers who will actually churn',
            'business_impact': 'Identifies most at-risk customers'
        },
        'inference_time_ms': {
            'target': 500,
            'description': 'Prediction must complete within 500ms',
            'business_impact': 'Enables real-time customer scoring'
        }
    }
    
    print("‚úÖ Success criteria defined:")
    for metric, details in success_criteria.items():
        print(f"   ‚Ä¢ {metric}: {details['target']} - {details['description']}")
    
    # 1.2 Establish Baseline
    print("\n1.2 üìä Baseline Establishment")
    print("-" * 40)
    
    baseline_metrics = {
        'current_process': {
            'method': 'Manual review by customer success team',
            'precision': 0.45,
            'recall': 0.60,
            'cost_per_prediction': 25.0,
            'description': 'Current manual process performance'
        },
        'simple_model': {
            'method': 'Rule-based system (tenure < 6 months = churn)',
            'precision': 0.55,
            'recall': 0.40,
            'cost_per_prediction': 0.10,
            'description': 'Simple automated baseline'
        }
    }
    
    print("‚úÖ Baselines established:")
    for baseline, details in baseline_metrics.items():
        print(f"   ‚Ä¢ {baseline}: Precision {details['precision']:.2f}, "
              f"Recall {details['recall']:.2f}")
    
    # 1.3 Data Scoping
    print("\n1.3 üóÉÔ∏è  Data Scoping Assessment")
    print("-" * 40)
    
    data_requirements = {
        'minimum_samples': 1000,
        'minimum_positive_class': 100,
        'required_features': ['tenure_months', 'monthly_charges', 'support_tickets'],
        'data_quality_threshold': 0.95,
        'time_range': '12_months'
    }
    
    print("‚úÖ Data requirements defined:")
    for requirement, value in data_requirements.items():
        print(f"   ‚Ä¢ {requirement}: {value}")
    
    return {
        'success_criteria': success_criteria,
        'baseline_metrics': baseline_metrics,
        'data_requirements': data_requirements
    }

def pillar_2_technical_validation(data: pd.DataFrame, strategic_results: Dict):
    """
    Pillar 2: Technical Validation (The "How" with Python)
    Model training, evaluation, and reproducibility
    """
    print("\n" + "="*60)
    print("üî¨ PILLAR 2: TECHNICAL VALIDATION (The 'How')")
    print("="*60)
    
    # 2.1 Data Splitting and Preparation
    print("\n2.1 üîÑ Data Preparation & Splitting")
    print("-" * 40)
    
    # Prepare features and target
    X = data.drop('churn', axis=1)
    y = data['churn']
    
    # Mock train_test_split if sklearn not available
    if ensure_sklearn_available():
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
    else:
        # Simple manual split
        split_idx = int(0.8 * len(data))
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    print(f"‚úÖ Data split completed:")
    print(f"   ‚Ä¢ Training set: {len(X_train)} samples")
    print(f"   ‚Ä¢ Test set: {len(X_test)} samples") 
    print(f"   ‚Ä¢ Train churn rate: {y_train.mean():.1%}")
    print(f"   ‚Ä¢ Test churn rate: {y_test.mean():.1%}")
    
    # 2.2 Model Training and Evaluation
    print("\n2.2 ü§ñ Model Training & Evaluation")
    print("-" * 40)
    
    # Train model
    if ensure_sklearn_available():
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.metrics import precision_score, recall_score, f1_score
        model = RandomForestClassifier(random_state=42, n_estimators=50)
    else:
        model = MockModel()
        
    print("üîß Training model...")
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    if ensure_sklearn_available():
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
    else:
        # Calculate manually
        tp = np.sum((y_test == 1) & (y_pred == 1))
        fp = np.sum((y_test == 0) & (y_pred == 1))
        fn = np.sum((y_test == 1) & (y_pred == 0))
        
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    metrics = {
        'precision': precision,
        'recall': recall,
        'f1_score': f1
    }
    
    print("üìä Model Performance:")
    print(f"   ‚Ä¢ Precision: {precision:.3f}")
    print(f"   ‚Ä¢ Recall: {recall:.3f}")
    print(f"   ‚Ä¢ F1-Score: {f1:.3f}")
    
    # 2.3 Success Criteria Validation
    print("\n2.3 üéØ Success Criteria Validation")
    print("-" * 40)
    
    success_criteria = strategic_results['success_criteria']
    validation_results = {}
    
    for metric in ['precision', 'recall']:
        achieved = metrics[metric]
        target = success_criteria[metric]['target']
        passed = achieved >= target
        
        validation_results[metric] = {
            'achieved': achieved,
            'target': target,
            'passed': passed
        }
        
        status = '‚úÖ PASS' if passed else '‚ùå FAIL'
        print(f"   ‚Ä¢ {metric.upper()}: {achieved:.3f} (target: {target:.3f}) {status}")
    
    overall_technical_success = all(r['passed'] for r in validation_results.values())
    print(f"\nüèÜ Technical Validation: {'‚úÖ PASSED' if overall_technical_success else '‚ùå FAILED'}")
    
    return {
        'model': model,
        'X_test': X_test,
        'y_test': y_test,
        'y_pred': y_pred,
        'metrics': metrics,
        'validation_results': validation_results,
        'overall_success': overall_technical_success
    }

def pillar_3_operational_validation(technical_results: Dict, strategic_results: Dict):
    """
    Pillar 3: Operational Validation (The "What If") 
    Performance, robustness, and production readiness
    """
    print("\n" + "="*60)
    print("‚ö° PILLAR 3: OPERATIONAL VALIDATION (The 'What If')")
    print("="*60)
    
    model = technical_results['model']
    X_test = technical_results['X_test']
    
    # 3.1 Performance Testing
    print("\n3.1 üöÄ Performance & Resource Testing")
    print("-" * 40)
    
    # Test inference time
    sample_instance = X_test.iloc[[0]]
    
    # Warm up
    _ = model.predict(sample_instance)
    
    # Measure inference times
    times = []
    for _ in range(10):  # Smaller sample for demo
        start_time = time.time()
        _ = model.predict(sample_instance)
        end_time = time.time()
        times.append((end_time - start_time) * 1000)  # Convert to ms
    
    avg_inference_time = np.mean(times)
    target_time = strategic_results['success_criteria']['inference_time_ms']['target']
    
    performance_passed = avg_inference_time <= target_time
    
    print(f"üìà Performance Metrics:")
    print(f"   ‚Ä¢ Average inference time: {avg_inference_time:.2f}ms")
    print(f"   ‚Ä¢ Target time: {target_time}ms")
    print(f"   ‚Ä¢ Throughput: ~{1000/avg_inference_time:.0f} predictions/second")
    print(f"   ‚Ä¢ Performance: {'‚úÖ PASS' if performance_passed else '‚ùå FAIL'}")
    
    # 3.2 Robustness Testing
    print("\n3.2 üõ°Ô∏è  Robustness Testing")
    print("-" * 40)
    
    robustness_results = {}
    
    # Test with missing values (simulate)
    print("Testing edge cases:")
    try:
        # Create edge case: very short tenure, very high charges
        edge_case = pd.DataFrame({
            'tenure_months': [1.0],
            'monthly_charges': [200.0],
            'support_tickets': [10],
            'total_charges': [200.0]
        })
        
        edge_prediction = model.predict(edge_case)
        robustness_results['handles_edge_cases'] = True
        print("   ‚Ä¢ Edge cases: ‚úÖ Model handles gracefully")
        
    except Exception as e:
        robustness_results['handles_edge_cases'] = False
        print(f"   ‚Ä¢ Edge cases: ‚ùå Model fails - {str(e)[:50]}")
    
    # Error analysis
    y_test = technical_results['y_test']
    y_pred = technical_results['y_pred']
    
    misclassified_count = np.sum(y_pred != y_test)
    total_count = len(y_test)
    error_rate = misclassified_count / total_count
    
    print(f"   ‚Ä¢ Error rate: {error_rate:.1%} ({misclassified_count}/{total_count})")
    print(f"   ‚Ä¢ Robustness: {'‚úÖ ACCEPTABLE' if error_rate < 0.4 else '‚ùå NEEDS IMPROVEMENT'}")
    
    # 3.3 Production Readiness Assessment
    print("\n3.3 üè≠ Production Readiness")
    print("-" * 40)
    
    operational_score = 0
    max_score = 3
    
    if performance_passed:
        operational_score += 1
        print("   ‚Ä¢ Performance requirements: ‚úÖ")
    else:
        print("   ‚Ä¢ Performance requirements: ‚ùå")
        
    if robustness_results.get('handles_edge_cases', False):
        operational_score += 1
        print("   ‚Ä¢ Edge case handling: ‚úÖ")
    else:
        print("   ‚Ä¢ Edge case handling: ‚ùå")
        
    if error_rate < 0.35:  # Acceptable error rate
        operational_score += 1
        print("   ‚Ä¢ Error rate acceptable: ‚úÖ")  
    else:
        print("   ‚Ä¢ Error rate acceptable: ‚ùå")
    
    operational_success = operational_score >= 2  # At least 2/3 criteria
    
    print(f"\nüèÜ Operational Validation: {'‚úÖ PASSED' if operational_success else '‚ùå FAILED'}")
    print(f"   Score: {operational_score}/{max_score}")
    
    return {
        'performance_results': {
            'avg_inference_time_ms': avg_inference_time,
            'target_time_ms': target_time,
            'performance_passed': performance_passed
        },
        'robustness_results': robustness_results,
        'error_rate': error_rate,
        'operational_success': operational_success
    }

def generate_final_recommendation(strategic_results: Dict, technical_results: Dict, 
                                operational_results: Dict):
    """Generate final PoC validation recommendation"""
    print("\n" + "="*60)
    print("üèÜ FINAL POC VALIDATION SUMMARY")
    print("="*60)
    
    strategic_passed = True  # Strategic validation always passes in this demo
    technical_passed = technical_results['overall_success']
    operational_passed = operational_results['operational_success']
    
    print(f"‚úÖ Strategic Validation:   {'‚úÖ PASSED' if strategic_passed else '‚ùå FAILED'}")
    print(f"‚úÖ Technical Validation:   {'‚úÖ PASSED' if technical_passed else '‚ùå FAILED'}")
    print(f"‚úÖ Operational Validation: {'‚úÖ PASSED' if operational_passed else '‚ùå FAILED'}")
    
    overall_success = strategic_passed and technical_passed and operational_passed
    
    print("\n" + "="*60)
    if overall_success:
        print("üéâ RECOMMENDATION: PROCEED TO MVP DEVELOPMENT")
        print("‚úÖ All validation pillars passed")
        print("‚úÖ Success criteria met")
        print("‚úÖ Technical feasibility confirmed")
        print("‚úÖ Operational readiness validated")
        
        print("\nüéØ Next Steps:")
        print("1. Begin MVP development planning")
        print("2. Set up production infrastructure")
        print("3. Design A/B testing framework")
        print("4. Establish monitoring systems")
    else:
        print("‚ö†Ô∏è  RECOMMENDATION: ADDITIONAL RESEARCH NEEDED")
        print("‚ùå One or more validation pillars failed")
        
        if not technical_passed:
            print("   ‚Ä¢ Address technical performance issues")
        if not operational_passed:
            print("   ‚Ä¢ Improve operational readiness")
            
        print("\nüîÑ Next Steps:")
        print("1. Address failed criteria")
        print("2. Gather additional data if needed")
        print("3. Experiment with different approaches")
        print("4. Re-run validation after improvements")
    
    return {
        'overall_success': overall_success,
        'strategic_passed': strategic_passed,
        'technical_passed': technical_passed,
        'operational_passed': operational_passed,
        'recommendation': 'PROCEED_TO_MVP' if overall_success else 'ADDITIONAL_RESEARCH'
    }

def main():
    """Run the complete AI PoC validation demonstration"""
    print("üß¨ AI PROOF OF CONCEPT VALIDATION DEMO")
    print("="*60)
    print("Demonstrating comprehensive 3-pillar validation methodology")
    print("Following guide: docs/AI_POC_VALIDATION_GUIDE.md")
    print("="*60)
    
    # Set random seed for reproducibility (following Bioart patterns)
    np.random.seed(42)
    print("üîí Reproducibility locked with seed: 42")
    
    # Create demonstration dataset
    data = create_synthetic_dataset()
    
    # Run three-pillar validation
    try:
        # Pillar 1: Strategic Validation
        strategic_results = pillar_1_strategic_validation()
        
        # Pillar 2: Technical Validation  
        technical_results = pillar_2_technical_validation(data, strategic_results)
        
        # Pillar 3: Operational Validation
        operational_results = pillar_3_operational_validation(technical_results, strategic_results)
        
        # Final Recommendation
        final_results = generate_final_recommendation(
            strategic_results, technical_results, operational_results
        )
        
        print(f"\nüìä VALIDATION COMPLETED SUCCESSFULLY!")
        print(f"üïí Demo executed with {len(data)} sample dataset")
        print(f"üìö Full methodology: docs/AI_POC_VALIDATION_GUIDE.md")
        
    except Exception as e:
        print(f"\n‚ùå Validation demo encountered an error: {e}")
        print("This is a demonstration script - in real validation, investigate and fix errors")
        
    print("\n" + "="*60)

if __name__ == "__main__":
    main()